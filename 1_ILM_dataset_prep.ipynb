{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f6b7aa",
   "metadata": {},
   "source": [
    "This notebook creates the dataset that we use to train an ILM model to explain positive predictions of a toxicity classifier. The datasets we chose are for the toxic/abusive language detection task, close to each other in their task definition and from a variety of different sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075cd040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#import preprocessor\n",
    "import pickle\n",
    "import wordsegment as ws\n",
    "from html import unescape\n",
    "import re\n",
    "import string\n",
    "ws.load() # load vocab for word segmentation\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "# Cleaning functions from hatecheck-experiments\n",
    "# Define helper function for segmenting hashtags found through regex\n",
    "def regex_match_segmentation(match):\n",
    "    return ' '.join(ws.segment(match.group(0)))\n",
    "\n",
    "# Define function for cleaning text\n",
    "def clean_text(text):\n",
    "    \n",
    "    # convert HTML codes\n",
    "    text = unescape(text)\n",
    "    \n",
    "    # lowercase text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # replace mentions, URLs and emojis with special token\n",
    "    text = re.sub(r\"@[A-Za-z0-9_-]+\",'[USER]',text)\n",
    "    text = re.sub(r\"u/[A-Za-z0-9_-]+\",'[USER]',text)\n",
    "    text = re.sub(r\"http\\S+\",'[URL]',text)\n",
    "    \n",
    "    # find and split hashtags into words\n",
    "    text = re.sub(r\"#[A-Za-z0-9]+\", regex_match_segmentation, text)\n",
    "\n",
    "    # remove punctuation at beginning of string (quirk in Davidson data)\n",
    "    text = text.lstrip(\"!\")\n",
    "    text = text.lstrip(\":\")\n",
    "    \n",
    "    # remove newline and tab characters\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.replace('\\t',' ')\n",
    "    text = text.replace('[linebreak]', ' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab0073f",
   "metadata": {},
   "source": [
    "## Founta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e5bf5",
   "metadata": {},
   "source": [
    "The first dataset we consider is from [Founta et al. 2018](https://arxiv.org/pdf/1802.00393.pdf), which is a dataset sampled from Twitter. We split this into train, valid and test sets here, and only use the neutral tweets in the train split to train the ILM. We will use the same splits later when training a BERT classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d82ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts = pd.read_csv(\"../Founta/hatespeech_text_label_vote.csv\",names=['text', 'label', 'count_label_votes'], delimiter='\\t')\n",
    "df_texts.drop_duplicates(subset='text', inplace=True)\n",
    "founta_train, founta_valtest = train_test_split(df_texts, test_size=0.2, stratify=df_texts.label, random_state=123)\n",
    "founta_val, founta_test = train_test_split(founta_valtest, test_size=0.5, stratify=founta_valtest.label, random_state=123)\n",
    "founta_train_neutral = founta_train[founta_train['label'] == 'normal']\n",
    "\n",
    "founta_train.to_csv(\"Data/Founta/train.csv\")\n",
    "founta_val.to_csv(\"Data/Founta/valid.csv\")\n",
    "founta_test.to_csv(\"Data/Founta/test.csv\")\n",
    "\n",
    "founta_train_neutral[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e454c011",
   "metadata": {},
   "source": [
    "## CAD\n",
    "\n",
    "Next, we get the neutral posts from the CAD dataset, introduced in [Vigden et al. 2021](https://aclanthology.org/2021.naacl-main.182.pdf) and can be obtained from [here](https://zenodo.org/record/4881008#.YnvpkvPMK3I). This dataset is sourced from Reddit, and posts are annotated with hierarchical labels, and within their context. For our task we only keep the posts with the Neutral label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bce221",
   "metadata": {},
   "outputs": [],
   "source": [
    "cad_train = pd.read_csv(\"../cad_naacl2021/data/cad_v1_1_train.tsv\", sep=\"\\t\")\n",
    "cad_train_neutral = cad_train[cad_train.labels == 'Neutral']\n",
    "cad_train_neutral[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568a5406",
   "metadata": {},
   "source": [
    "## Wikipedia Toxicity\n",
    "\n",
    "The next dataset we use is the Wikipedia Toxicity dataset from [Wulczyn et al. 2017](https://arxiv.org/abs/1610.08914), which can be downloaded [here](https://figshare.com/articles/dataset/Wikipedia_Talk_Labels_Toxicity/4563973). As shown in [Nejadgholi and Kiritchenko 2020](https://aclanthology.org/2020.alw-1.20.pdf), the neutral class for this dataset is dominated by Wikipedia specific topics such as edits and formatting. We use the topic clusters found in this work to remove these domain specific instances from the training set before sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2cb801",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('../cross_dataset_toxicity/toxicity_annotated_comments.tsv', sep = '\\t', index_col = 0)  #from https://figshare.com/articles/dataset/Wikipedia_Talk_Labels_Toxicity/4563973\n",
    "annotations = pd.read_csv('../cross_dataset_toxicity/toxicity_annotations.tsv',  sep = '\\t')\n",
    "# join labels and comments\n",
    "comments['toxicity'] = annotations.groupby('rev_id')['toxicity'].mean() > 0.5\n",
    "\n",
    "# # remove newline and tab tokens\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "\n",
    "wiki_topics = pd.read_csv('../cross_dataset_toxicity/wiki_toxicity_topics.csv', index_col=[0]) #from this repo\n",
    "\n",
    "data = comments.merge(wiki_topics, on='rev_id')  #merge the two datasets\n",
    "\n",
    "#pruned Wiki-toxic \n",
    "topic_categories={1:[0,1],\n",
    "                  2:[2,7,8,9,12,14,16],\n",
    "                  3:[3,4,5,6,10,11,13,15,17,18,19]}\n",
    "\n",
    "\n",
    "toxic_train_pruned = data[data['split']=='train' ][data['wiki_topic'].isin(topic_categories[1]+topic_categories[2])]\n",
    "wiki_train_neutral = toxic_train_pruned[toxic_train_pruned.toxicity == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e5f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train_neutral[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792215e0",
   "metadata": {},
   "source": [
    "## Civil Comments\n",
    "\n",
    "Next, we get the civil_comments from [kaggle](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data). This dataset consists of comments made on a number of\n",
    "news platforms, within the years 2015-2017, and later annotated by Jigsaw. For picking neutral comments, we pick the comments where the target is 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fa6197",
   "metadata": {},
   "outputs": [],
   "source": [
    "civil_comments_train = pd.read_csv('../civil_comments/train.csv')\n",
    "civil_comments_neutral = civil_comments_train[(civil_comments_train['target'] < 0.0001)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ae8122",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2547e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing the sizes of different datasets\n",
    "len(founta_train_neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f80ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cad_train_neutral.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afbfa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_train_neutral.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "civil_comments_neutral.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 30K comments from civil_comments, and take others as is. \n",
    "civil_comments_sampled = civil_comments_neutral.sample(n=30000, random_state=random_seed)\n",
    "civil_comments_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21670905",
   "metadata": {},
   "outputs": [],
   "source": [
    "civil_comments_sampled['comment_text'] = civil_comments_sampled['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "founta_texts = [clean_text(tt) for tt in founta_train_neutral['text'].tolist()]\n",
    "cad_texts = [clean_text(tt) for tt in cad_train_neutral['text'].tolist()]\n",
    "wiki_texts = [clean_text(tt) for tt in wiki_train_neutral['comment'].tolist()]\n",
    "civil_texts = [clean_text(tt) for tt in civil_comments_sampled['comment_text'].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29253781",
   "metadata": {},
   "source": [
    "We divide the texts again to train valid and test splits for the ILM training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d877b37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from random import Random\n",
    "\n",
    "founta_train, founta_valid = train_test_split(founta_texts, test_size=0.05, random_state=random_seed+1)\n",
    "cad_train, cad_valid = train_test_split(cad_texts, test_size=0.05, random_state=random_seed+2)\n",
    "wiki_train, wiki_valid = train_test_split(wiki_texts, test_size=0.05, random_state=random_seed+3)\n",
    "civil_train, civil_valid = train_test_split(wiki_texts, test_size=0.05, random_state=random_seed+4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c394f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_train = founta_train + cad_train + wiki_train + civil_train\n",
    "compound_valid = founta_valid + cad_valid + wiki_valid + civil_valid\n",
    "Random(random_seed+5).shuffle(compound_train)\n",
    "Random(random_seed+6).shuffle(compound_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f253ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/ILM/compound_dataset/train.txt\", \"w\") as ff:\n",
    "    ff.write(\"\\n\\n\\n\".join(compound_train))\n",
    "    \n",
    "with open(\"Data/ILM/compound_dataset/valid.txt\", \"w\") as ff:\n",
    "    ff.write(\"\\n\\n\\n\".join(compound_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd87ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
