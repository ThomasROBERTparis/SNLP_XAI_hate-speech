{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43fb9d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2021/thomas.robert.x21/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/users/eleves-b/2021/thomas.robert.x21/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from random import shuffle\n",
    "from perturbation_functions import get_preds_and_scores, calc_suff, calc_necc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e44157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['orig_texts', 'necc_perturbed', 'suff_perturbed', 'necc_masks', 'suff_masks'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "# perts = pickle.load(open(\"Data/intermediate outputs/HateCheck_necc_suff_perturbations.pickle\",\"rb\"))\n",
    "# perts = pickle.load(open(\"Data/Reproduction/HateCheck_necc_suff_perturbations_0_20.pickle\",\"rb\"))\n",
    "perts = pickle.load(open(\"Data/Reproduction_full/HateCheck_necc_suff_perturbations_not_finetuned.pickle\",\"rb\"))\n",
    "###\n",
    "\n",
    "perts['orig_texts'] = [tt.strip(' \\n') for tt in perts['orig_texts']]\n",
    "perts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b9c151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd68c9591844441b0e6002aa5618f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.82k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462d92723af84939a45392fb9972b981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5b1e3840954012a51980529202290d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059135cf0ec24bd28a28eb4159973984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d61fadc5e74ce0b6311e7870d77b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/582 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8721d3b718c349dda4bfbeae5410f7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/680 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0914e35d2348dcb2aebcf6ee3a9a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# model_name = \"ThomasROBERTparis/SNLP_XAI_hate-speech_Davidson_hate_bert-base-uncased_lr5e-05\"\n",
    "model_name = \"ThomasROBERTparis/SNLP_XAI_hate-speech_Davidson_hate_bert-large-uncased_lr0.0001\"\n",
    "# model_name = \"ThomasROBERTparis/SNLP_XAI_hate-speech_Davidson_hate_roberta-base_lr5e-05\"\n",
    "# model_name = \"ThomasROBERTparis/SNLP_XAI_hate-speech_Davidson_hate_distilbert-base-uncased_lr0.0001\"\n",
    "# model_name = \"ThomasROBERTparis/SNLP_XAI_hate-speech_Davidson_hate_distilroberta-base_lr2e-05\"\n",
    "\n",
    "# model_name = \"ThomasROBERTparis/SNLP_XAI_hate-speech_Davidson_hate_bert-base-uncased_LoRA_r16_lr0.0001\"\n",
    "# model_name = \"ThomasROBERTparis/SNLP_XAI_hate-speech_Davidson_hate_bert-large-uncased_LoRA_r16_lr0.0001\"\n",
    "# Dmodel_name = \"ThomasROBERTparis/SNLP_XAI_hate-speech_Davidson_hate_roberta-base_LoRA_r16_lr2e-05\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.eval()\n",
    "\n",
    "# add special tokens for URLs, emojis and mentions (--> see pre-processing)\n",
    "special_tokens_dict = {'additional_special_tokens': ['[USER]','[EMOJI]','[URL]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# datasets = ['CAD_abuse', \n",
    "#             'Davidson_abuse', \n",
    "#             'Founta_abuse',\n",
    "#             'CAD_hate',\n",
    "#             'Davidson_hate',\n",
    "#             'Founta_hate']\n",
    "datasets = [\"Davidson_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a000c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying HateCheck perturbations with Davidson_hate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed69a1fea5b4fe4a10358d7cf82115a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/132480 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "orig_preds = {}\n",
    "orig_scores = {}\n",
    "necc_preds = {}\n",
    "necc_scores = {}\n",
    "suff_preds = {}\n",
    "suff_scores = {}\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Classifying HateCheck perturbations with {}.\".format(dataset))\n",
    "    # model = BertForSequenceClassification.from_pretrained(\"Model/{}\".format(dataset))\n",
    "    # model.resize_token_embeddings(len(tokenizer))\n",
    "    # model.eval()\n",
    "    \n",
    "    total_len = len(perts['orig_texts']) + sum(len(nn) for nn in perts['necc_perturbed']) + sum(len(nn) for nn in perts['suff_perturbed'])\n",
    " \n",
    "    with tqdm(total=total_len) as pbar:\n",
    "        orig_preds[dataset], orig_scores[dataset] = get_preds_and_scores(perts['orig_texts'], tokenizer, model, pbar)\n",
    "        \n",
    "        necc_preds[dataset] = []\n",
    "        necc_scores[dataset] = []\n",
    "    \n",
    "        for tt in perts['necc_perturbed']:\n",
    "            pp, ss = get_preds_and_scores(tt, tokenizer, model, pbar)\n",
    "            necc_preds[dataset].append(pp)\n",
    "            necc_scores[dataset].append(ss)\n",
    "            \n",
    "        suff_preds[dataset] = []\n",
    "        suff_scores[dataset] = []\n",
    "    \n",
    "        for tt in perts['suff_perturbed']:\n",
    "            pp, ss = get_preds_and_scores(tt, tokenizer, model, pbar)\n",
    "            suff_preds[dataset].append(pp)\n",
    "            suff_scores[dataset].append(ss)\n",
    "            \n",
    "        \n",
    "final_results = {\n",
    "                'orig_preds': orig_preds,\n",
    "                'orig_scores': orig_scores,\n",
    "                'necc_preds': necc_preds,\n",
    "                'necc_scores': necc_scores,\n",
    "                'suff_preds': suff_preds,\n",
    "                'suff_scores': suff_scores,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4837fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"Data/Reproduction_full/\"+model_name\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "pickle.dump(final_results, open(\"Data/Reproduction_full/\"+model_name+\"/HateCheck_necc_suff_preds.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b5706",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/ILM/compound_dataset/train.txt\", \"r\") as ff:\n",
    "    compound_dataset = ff.read().split(\"\\n\\n\\n\")\n",
    "compound_dataset = [tt.strip(\" :`.,\") for tt in compound_dataset]\n",
    "shuffle(compound_dataset)\n",
    "compound_dataset = compound_dataset[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e0a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_preds = {}\n",
    "baseline_scores = {}\n",
    "for dataset in datasets: \n",
    "    # model = BertForSequenceClassification.from_pretrained(\"Models/Classifiers/{}\".format(dataset))\n",
    "    # model.resize_token_embeddings(len(tokenizer))\n",
    "    # model.eval()\n",
    "    preds, scores = get_preds_and_scores(compound_dataset, tokenizer, model)\n",
    "    baseline_preds[dataset] = sum(preds)/len(preds)\n",
    "    baseline_scores[dataset] = sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e9ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump({'baseline_preds':baseline_preds, 'baseline_scores':baseline_scores}, open(\"Data/Reproduction_full/\"+model_name+\"/Classifier_baselines.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "necc_results = {}\n",
    "necc_results_nb = {}\n",
    "suff_results = {}\n",
    "suff_results_nb = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    ## NECCESSITY CALCULATIONS\n",
    "    neccs = []\n",
    "    for oo, pp, mm in zip(final_results['orig_preds'][dataset], \n",
    "                          final_results['necc_preds'][dataset], \n",
    "                          perts['necc_masks']):\n",
    "        pp = np.array(pp)\n",
    "        neccs.append(calc_necc(oo, pp, mm))\n",
    "    necc_results[dataset] = neccs \n",
    "    \n",
    "    neccs_nb = []\n",
    "    for oo, pp, mm in zip(final_results['orig_scores'][dataset], \n",
    "                          final_results['necc_scores'][dataset], \n",
    "                          perts['necc_masks']):\n",
    "        pp = np.array(pp)\n",
    "        neccs_nb.append(calc_necc(oo, pp, mm))\n",
    "    necc_results_nb[dataset] = neccs_nb\n",
    "    \n",
    "    ## SUFFICIENCY CALCULATIONS\n",
    "    baseline_pred = baseline_preds[dataset]\n",
    "    baseline_score = baseline_scores[dataset]\n",
    "    \n",
    "    suffs = []\n",
    "    for pp, mm in zip(final_results['suff_preds'][dataset], perts['suff_masks']):\n",
    "        pp = np.array(pp)\n",
    "        suffs.append(calc_suff(baseline_pred, pp, mm))\n",
    "    suff_results[dataset] = suffs \n",
    "    \n",
    "    suffs_nb = []\n",
    "    for pp, mm in zip(final_results['suff_scores'][dataset], perts['suff_masks']):\n",
    "        pp = np.array(pp)\n",
    "        suffs_nb.append(calc_suff(baseline_score, pp, mm))\n",
    "    suff_results_nb[dataset] = suffs_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23175599",
   "metadata": {},
   "outputs": [],
   "source": [
    "hatecheck_necc_suff_results = {\n",
    "    'necc_results': necc_results,\n",
    "    'necc_results_nb': necc_results_nb,\n",
    "    'suff_results': suff_results, \n",
    "    'suff_results_nb': suff_results_nb\n",
    "}\n",
    "\n",
    "pickle.dump(hatecheck_necc_suff_results, open(\"Data/Reproduction_full/\"+model_name+\"/HateCheck_necc_suff_results_all.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a850153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now get the predictions for all models for the entire hatecheck suite\n",
    "###\n",
    "hc_test_cases_all = pd.read_csv(\"hatecheck-data_reproduction/test_suite_cases_not_finetuned.csv\")\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b6ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['women', 'Catholics', 'trans people', 'men', 'gay people',\n",
       "       'black people', 'disabled people', 'Muslims', 'immigrants', nan],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc_test_cases_all.target_ident.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_test_cases_all_list = hc_test_cases_all.test_case.tolist()\n",
    "hc_preds = {}\n",
    "hc_scores = {}\n",
    "for dataset in datasets: \n",
    "    # model = BertForSequenceClassification.from_pretrained(\"Models/Classifiers/{}\".format(dataset))\n",
    "    # model.resize_token_embeddings(len(tokenizer))\n",
    "    # model.eval()\n",
    "    preds, scores = get_preds_and_scores(hc_test_cases_all_list, tokenizer, model)\n",
    "    hc_preds[dataset] = preds\n",
    "    hc_scores[dataset] = scores\n",
    "\n",
    "###\n",
    "# pickle.dump({'preds': hc_preds, 'scores':hc_scores}, open('Data/HateCheck_results_all_models.pickle', \"wb\"))\n",
    "pickle.dump({'preds': hc_preds, 'scores':hc_scores}, open(\"Data/Reproduction_full/\"+model_name+\"/HateCheck_results_all_models.pickle\", \"wb\"))\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19e934",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Then this should work\n",
    "for dataset in datasets:\n",
    "    hc_test_cases_all['{}_pred'.format(dataset)] = hc_preds[dataset]\n",
    "    hc_test_cases_all['{}_score'.format(dataset)] = hc_scores[dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31924981",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# pickle.dump(hc_test_cases_all, open('Data/HateCheck_templates_and_results.pickle', \"wb\"))\n",
    "pickle.dump(hc_test_cases_all, open(\"Data/Reproduction_full/\"+model_name+\"/HateCheck_templates_and_results.pickle\", \"wb\"))\n",
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
