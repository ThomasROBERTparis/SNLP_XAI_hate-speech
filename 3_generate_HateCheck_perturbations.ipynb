{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1876462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "import ilm.ilm.tokenize_util\n",
    "from ilm.ilm.infer import infill_with_ilm\n",
    "from perturbation_functions import calculate_necc_and_suff, gen_num_samples_table, gen_probs_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5735037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_DIR = './ilm/Models/ILM/'\n",
    "# MASK_CLS = 'ilm.ilm.mask.hierarchical.MaskHierarchical'\n",
    "\n",
    "# tokenizer = ilm.ilm.tokenize_util.Tokenizer.GPT2\n",
    "# with open(os.path.join(MODEL_DIR, 'additional_ids_to_tokens.pkl'), 'rb') as f:\n",
    "#     additional_ids_to_tokens = pickle.load(f)\n",
    "# additional_tokens_to_ids = {v:k for k, v in additional_ids_to_tokens.items()}\n",
    "# try:\n",
    "#     ilm.ilm.tokenize_util.update_tokenizer(additional_ids_to_tokens, tokenizer)\n",
    "# except ValueError:\n",
    "#     print('Already updated')\n",
    "# print(additional_tokens_to_ids)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = GPT2LMHeadModel.from_pretrained(MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b81a07ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already updated\n",
      "{'<|startofinfill|>': 50257, '<|endofinfill|>': 50258, '<|infill_document|>': 50259, '<|infill_paragraph|>': 50260, '<|infill_sentence|>': 50261, '<|infill_ngram|>': 50262, '<|infill_word|>': 50263}\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = './ilm/Models/ILM/'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, 'additional_ids_to_tokens.pkl'), 'rb') as f:\n",
    "    additional_ids_to_tokens = pickle.load(f)\n",
    "additional_tokens_to_ids = {v:k for k, v in additional_ids_to_tokens.items()}\n",
    "try:\n",
    "    ilm.ilm.tokenize_util.update_tokenizer(additional_ids_to_tokens, tokenizer)\n",
    "except ValueError:\n",
    "    print('Already updated')\n",
    "print(additional_tokens_to_ids)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.eval()\n",
    "_ = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542ad558",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_suite_cases = pd.read_csv(\"hatecheck-data/test_suite_cases.csv\", index_col=\"case_id\") #.drop(columns=['Unnamed:0'])\n",
    "test_suite_cases.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "# target_ds = ['women', 'Muslims']\n",
    "# funcs = ['derog_neg_emote_h', 'derog_neg_attrib_h', 'derog_dehum_h']\n",
    "# \n",
    "# test_suite_cases = test_suite_cases[test_suite_cases.target_ident.isin(target_ds) & \n",
    "                                    # test_suite_cases.functionality.isin(funcs)]\n",
    "# tts = [text for _, text in test_suite_cases.test_case.items()]\n",
    "# \n",
    "# with open(\"Data/HateCheck_test_suite_cases.txt\", \"w\") as f:\n",
    "    # f.write(\"\\n\".join(tts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8b8559da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['women', 'trans people', 'gay people', 'black people',\n",
       "       'disabled people', 'Muslims', 'immigrants', nan], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_suite_cases[\"target_ident\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "560090d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>functionality</th>\n",
       "      <th>test_case</th>\n",
       "      <th>label_gold</th>\n",
       "      <th>target_ident</th>\n",
       "      <th>direction</th>\n",
       "      <th>focus_words</th>\n",
       "      <th>focus_lemma</th>\n",
       "      <th>ref_case_id</th>\n",
       "      <th>ref_templ_id</th>\n",
       "      <th>templ_id</th>\n",
       "      <th>case_templ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>I hate women.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>women</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>I hate trans people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>trans people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>I hate gay people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>gay people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>I hate black people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>black people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>I hate disabled people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>disabled people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>I hate Muslims.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>Muslims</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>I hate immigrants.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>immigrants</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>I really can't stand women like you.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>women</td>\n",
       "      <td>directed</td>\n",
       "      <td>can't stand</td>\n",
       "      <td>can't stand</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>I really can't stand [IDENTITY_P] like you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>I really can't stand trans people like you.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>trans people</td>\n",
       "      <td>directed</td>\n",
       "      <td>can't stand</td>\n",
       "      <td>can't stand</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>I really can't stand [IDENTITY_P] like you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>I really can't stand gay people like you.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>gay people</td>\n",
       "      <td>directed</td>\n",
       "      <td>can't stand</td>\n",
       "      <td>can't stand</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>I really can't stand [IDENTITY_P] like you.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             functionality                                     test_case  \\\n",
       "case_id                                                                    \n",
       "1        derog_neg_emote_h                                I hate women.    \n",
       "2        derog_neg_emote_h                         I hate trans people.    \n",
       "3        derog_neg_emote_h                           I hate gay people.    \n",
       "4        derog_neg_emote_h                         I hate black people.    \n",
       "5        derog_neg_emote_h                      I hate disabled people.    \n",
       "6        derog_neg_emote_h                              I hate Muslims.    \n",
       "7        derog_neg_emote_h                           I hate immigrants.    \n",
       "8        derog_neg_emote_h         I really can't stand women like you.    \n",
       "9        derog_neg_emote_h  I really can't stand trans people like you.    \n",
       "10       derog_neg_emote_h    I really can't stand gay people like you.    \n",
       "\n",
       "        label_gold     target_ident direction  focus_words  focus_lemma  \\\n",
       "case_id                                                                   \n",
       "1          hateful            women   general         hate         hate   \n",
       "2          hateful     trans people   general         hate         hate   \n",
       "3          hateful       gay people   general         hate         hate   \n",
       "4          hateful     black people   general         hate         hate   \n",
       "5          hateful  disabled people   general         hate         hate   \n",
       "6          hateful          Muslims   general         hate         hate   \n",
       "7          hateful       immigrants   general         hate         hate   \n",
       "8          hateful            women  directed  can't stand  can't stand   \n",
       "9          hateful     trans people  directed  can't stand  can't stand   \n",
       "10         hateful       gay people  directed  can't stand  can't stand   \n",
       "\n",
       "         ref_case_id  ref_templ_id  templ_id  \\\n",
       "case_id                                        \n",
       "1                NaN           NaN         1   \n",
       "2                NaN           NaN         1   \n",
       "3                NaN           NaN         1   \n",
       "4                NaN           NaN         1   \n",
       "5                NaN           NaN         1   \n",
       "6                NaN           NaN         1   \n",
       "7                NaN           NaN         1   \n",
       "8                NaN           NaN         2   \n",
       "9                NaN           NaN         2   \n",
       "10               NaN           NaN         2   \n",
       "\n",
       "                                          case_templ  \n",
       "case_id                                               \n",
       "1                               I hate [IDENTITY_P].  \n",
       "2                               I hate [IDENTITY_P].  \n",
       "3                               I hate [IDENTITY_P].  \n",
       "4                               I hate [IDENTITY_P].  \n",
       "5                               I hate [IDENTITY_P].  \n",
       "6                               I hate [IDENTITY_P].  \n",
       "7                               I hate [IDENTITY_P].  \n",
       "8        I really can't stand [IDENTITY_P] like you.  \n",
       "9        I really can't stand [IDENTITY_P] like you.  \n",
       "10       I really can't stand [IDENTITY_P] like you.  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_suite_cases.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a4a712c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "553"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_suite_cases[pd.notna(test_suite_cases['target_ident'])]['case_templ'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33279e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "845"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da407dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word_hatecheck(word:str, df:pd.DataFrame):\n",
    "    indices = []\n",
    "    for x in df.loc[df['target_ident']!='nan'].unique():\n",
    "        index = int((df[\"case_templ\"] == x).idxmax())\n",
    "        indices.append(index)\n",
    "    return indices\n",
    "\n",
    "li = add_word_hatecheck('men', test_suite_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21d40325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 8, 15, 22, 29, 36, 43, 50, 57, 64, 71, 78, 85, 92, 99, 106, 113, 120, 127, 134, 141, 148, 155, 162, 169, 176, 183, 190, 197, 204, 211, 218, 225, 232, 239, 246, 253, 260, 267, 274, 281, 288, 295, 302, 309, 316, 323, 330, 337, 344, 351, 358, 365, 372, 379, 386, 393, 400, 407, 414, 421, 428, 435, 442, 449, 456, 463, 470, 477, 484, 491, 498, 505, 512, 519, 526, 533, 540, 547, 554, 561, 568, 575, 582, 589, 596, 603, 610, 617, 631, 638, 645, 652, 659, 666, 673, 680, 687, 694, 701, 708, 715, 722, 729, 736, 743, 750, 757, 764, 771, 778, 785, 792, 799, 806, 813, 820, 827, 834, 841, 859, 895, 913, 931, 949, 985, 1003, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1139, 1146, 1153, 1160, 1167, 1174, 1181, 1188, 1195, 1202, 1209, 1216, 1223, 1230, 1237, 1244, 1251, 1258, 1265, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1379, 1386, 1393, 1400, 1407, 1414, 1421, 1428, 1435, 1442, 1449, 1456, 1463, 1470, 1477, 1484, 1491, 1498, 1505, 1512, 1519, 1526, 1533, 1540, 1547, 1554, 1561, 1568, 1575, 1589, 1596, 1603, 1610, 1617, 1624, 1631, 1638, 1645, 1652, 1659, 1666, 1673, 1680, 1687, 1694, 1701, 1708, 1715, 1722, 1729, 1736, 1743, 1750, 1757, 1764, 1771, 1778, 1785, 1792, 1799, 1806, 1820, 1827, 1834, 1841, 1848, 1855, 1862, 1869, 1876, 1883, 1890, 1897, 1904, 1911, 1918, 1925, 1932, 1939, 1946, 1953, 1960, 1967, 1974, 1981, 1988, 1995, 2002, 2009, 2016, 2023, 2030, 2037, 2044, 2051, 2058, 2065, 2072, 2079, 2086, 2093, 2100, 2107, 2121, 2128, 2135, 2142, 2149, 2156, 2163, 2170, 2177, 2184, 2191, 2198, 2205, 2219, 2226, 2233, 2240, 2247, 2254, 2268, 2275, 2282, 2289, 2296, 2303, 2310, 2317, 2324, 2331, 2338, 2345, 2352, 2359, 2373, 2380, 2387, 2394, 2401, 2408, 2415, 2422, 2429, 2436, 2443, 2450, 2457, 2464, 2471, 2478, 2485, 2492, 2499, 2506, 2520, 2527, 2534, 2541, 2548, 2562, 2569, 2576, 2583, 2590, 2597, 2604, 2611, 2618, 2625, 2632, 2639, 2646, 2653, 2660, 2678, 2696, 2714, 2721, 2728, 2735, 2742, 2749, 2756, 2763, 2770, 2777, 2784, 2791, 2798, 2805, 2812, 2833, 2869, 2887, 2894, 2901, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3110, 3117, 3124, 3131, 3138, 3145, 3152, 3159, 3173, 3180, 3187, 3194, 3201, 3208, 3215, 3222, 3229, 3236, 3243, 3250, 3257, 3264, 3271, 3278, 3285, 3292, 3299, 3306, 3313, 3320, 3327, 3334, 3341, 3348, 3355, 3362, 3369, 3376, 3383, 3390, 3397, 3404, 3411, 3418, 3425, 3439, 3453, 3460, 3467, 3474, 3481, 3506, 3524, 3542, 3549, 3556, 3563, 3570, 3577, 3584, 3591, 3598, 3605, 3612, 3619, 3626, 3633, 3640, 3647, 3654, 3672, 3690, 3708, 3715, 3722, 3729, 3736, 3743, 3750, 3757, 3764, 3771, 3778, 3785, 3792, 3799, 3806, 3813, 3820, 3827, 3834, 3852, 3870, 3888, 3895]\n"
     ]
    }
   ],
   "source": [
    "print(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8805425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate approximately 100 perturbations for each token. \n",
    "num_samples = gen_num_samples_table(20, 100)\n",
    "probs_table = gen_probs_table(20)\n",
    "mask_tokn = additional_tokens_to_ids['<|infill_ngram|>']\n",
    "\n",
    "orig_texts = []\n",
    "necc_perturbed = []\n",
    "suff_perturbed = []\n",
    "necc_masks = []\n",
    "suff_masks = []\n",
    "\n",
    "with open(\"Data/HateCheck_test_suite_cases.txt\", \"r\") as ff:\n",
    "    with tqdm(total=120) as pbar:\n",
    "        for text in ff:\n",
    "            necc_pp, suff_pp, necc_mm, suff_mm = calculate_necc_and_suff(text, ilm_tokenizer=tokenizer, ilm_model=model, cl_tokenizer=None, cl_model=None, num_samples=num_samples,\n",
    "                               mask_tokn=mask_tokn, additional_tokens_to_ids=additional_tokens_to_ids, probs_table=probs_table, \n",
    "                               return_pert_only=True)\n",
    "\n",
    "            orig_texts.append(text)\n",
    "            necc_perturbed.append(necc_pp)\n",
    "            suff_perturbed.append(suff_pp)\n",
    "            necc_masks.append(necc_mm)\n",
    "            suff_masks.append(suff_mm)\n",
    "            pbar.update(1)\n",
    "    \n",
    "necc_suff_perturbations = {'orig_texts': orig_texts, \n",
    "                           'necc_perturbed': necc_perturbed, \n",
    "                           'suff_perturbed': suff_perturbed,\n",
    "                           'necc_masks': necc_masks,\n",
    "                           'suff_masks': suff_masks}\n",
    "\n",
    "#pickle.dump(necc_suff_perturbations, open('Data/HateCheck_necc_suff_perturbations.pickle', 'wb'))\n",
    "pickle.dump(necc_suff_perturbations, open('Data/HateCheck_necc_suff_perturbations_2.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd6edd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/hatecheck_perturbations/orig_texts.txt\", \"w\") as ff:\n",
    "    ff.write(\"\\n\".join(necc_suff_perturbations['orig_texts']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/hatecheck_perturbations/necc_perturbations.tsv\", \"w\") as ff:\n",
    "    for ll in necc_suff_perturbations['necc_perturbed']:\n",
    "        ff.write(\"\\t\".join(ll))\n",
    "        ff.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6efc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/hatecheck_perturbations/suff_perturbations.tsv\", \"w\") as ff:\n",
    "    for ll in necc_suff_perturbations['suff_perturbed']:\n",
    "        ff.write(\"\\t\".join(ll))\n",
    "        ff.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585c84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/hatecheck_perturbations/necc_masks.tsv\", \"w\") as ff: \n",
    "    for ll in necc_suff_perturbations['necc_masks']:\n",
    "        llist = ll.astype(int).astype(str).tolist()\n",
    "        ff.write(\"\\t\".join([\" \".join(ii) for ii in llist]))\n",
    "        ff.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df47b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/hatecheck_perturbations/suff_masks.tsv\", \"w\") as ff: \n",
    "    for ll in necc_suff_perturbations['suff_masks']:\n",
    "        llist = ll.astype(int).astype(str).tolist()\n",
    "        ff.write(\"\\t\".join([\" \".join(ii) for ii in llist]))\n",
    "        ff.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
