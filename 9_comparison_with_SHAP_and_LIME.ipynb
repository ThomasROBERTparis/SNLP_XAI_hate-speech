{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8866a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, pipeline\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f1fa4",
   "metadata": {},
   "source": [
    "## SHAP calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcc02eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# add special tokens for URLs, emojis and mentions (--> see pre-processing)\n",
    "special_tokens_dict = {'additional_special_tokens': ['[USER]','[EMOJI]','[URL]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396298e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/HateCheck_test_suite_cases.txt\", \"r\") as ff: \n",
    "    hatecheck_cases = ff.read().splitlines()\n",
    "    \n",
    "targets = ['women', 'Muslims']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c2f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['Founta_abuse', 'Founta_hate', 'Davidson_abuse', 'Davidson_hate', 'CAD_abuse', 'CAD_hate']\n",
    "\n",
    "shap_scores = {}\n",
    "shap_tokens = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Processing {}\".format(dataset))\n",
    "    model = BertForSequenceClassification.from_pretrained(\"Models/Classifiers/{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "    classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "    explainer = shap.Explainer(classifier)\n",
    "  #  preds = classifier(hatecheck_cases)\n",
    "  #  pos_cases = [tt for tt, pp in zip(hatecheck_cases, preds) if pp['label'] == 'LABEL_1']\n",
    "    shap_values = explainer(hatecheck_cases)\n",
    "    shap_tokens[dataset] = [[ww.strip().lower() for ww in list(dd)] for dd in shap_values.data]\n",
    "    shap_scores[dataset] = shap_values.values.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {dd:[] for dd in datasets}\n",
    "targets = {dd:[] for dd in datasets}\n",
    "\n",
    "for dataset in datasets:\n",
    "    for tt, ss in zip(shap_tokens[dataset], shap_scores[dataset]):\n",
    "        if 'women' in tt:\n",
    "            scores[dataset].append(ss[tt.index('women'), 1])\n",
    "            targets[dataset].append('women')\n",
    "        elif 'woman' in tt:\n",
    "            scores[dataset].append(ss[tt.index('woman'), 1])\n",
    "            targets[dataset].append('women')\n",
    "        elif 'female' in tt:\n",
    "            scores[dataset].append(ss[tt.index('female'), 1])\n",
    "            targets[dataset].append('women')\n",
    "        elif 'muslims' in tt:\n",
    "            scores[dataset].append(ss[tt.index('muslims'), 1])\n",
    "            targets[dataset].append('Muslims')\n",
    "        elif 'muslim' in tt:\n",
    "            scores[dataset].append(ss[tt.index('muslim'), 1])\n",
    "            targets[dataset].append('Muslims')\n",
    "        else:\n",
    "            raise ValueError(\"Didn't find either target in {}\".format(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a16706",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {('shap', dd): scores[dd] for dd in datasets}\n",
    "ind = [(tt, xx.strip()) for xx, tt in zip(hatecheck_cases, targets['Founta_hate'])]\n",
    "shap_df = pd.DataFrame(df_dict, index=ind)\n",
    "shap_df.columns = pd.MultiIndex.from_tuples(shap_df.columns, names=['value','Dataset'])\n",
    "shap_df.index = pd.MultiIndex.from_tuples(shap_df.index, names=['target', 'text'])\n",
    "shap_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14f03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(shap_df, open(\"Data/shap_scores.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f07815",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pickle.load(open(\"Data/HateCheck_individual_necc_suff_scores.pickle\", \"rb\"))\n",
    "joint_df = pd.merge(master_df, shap_df, left_index=True, right_index=True)\n",
    "joint_df = joint_df[joint_df[\"prediction\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe83ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_df['shap'].groupby(level='target').mean().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7222ac55",
   "metadata": {},
   "source": [
    "## LIME calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef30f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Founta_abuse\"\n",
    "model = BertForSequenceClassification.from_pretrained(\"Models/Classifiers/{}\".format(dataset))\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.eval()\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def get_probas(classifier_output):\n",
    "    probas = [dd['score'] for dd in classifier_output]\n",
    "    probas = [[1-pp, pp] for pp in probas]\n",
    "    return np.array(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae9dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['Founta_abuse', 'Founta_hate', 'Davidson_abuse', 'Davidson_hate', 'CAD_abuse', 'CAD_hate']\n",
    "\n",
    "lime_explanations = {}\n",
    "\n",
    "def get_probas(classifier_output):\n",
    "    probas = [dd['score'] for dd in classifier_output]\n",
    "    probas = [[1-pp, pp] for pp in probas]\n",
    "    return np.array(probas)\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Processing {}\".format(dataset))\n",
    "    model = BertForSequenceClassification.from_pretrained(\"Models/Classifiers/{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "    classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "    predictor = lambda x: get_probas(classifier(x))\n",
    "    explainer = LimeTextExplainer(class_names=['0','1'])\n",
    "    lime_explanations[dataset] = [explainer.explain_instance(ii, predictor).as_list() for ii in hatecheck_cases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac25385",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {('lime', dd): scores[dd] for dd in datasets}\n",
    "ind = [(tt, xx.strip()) for xx, tt in zip(hatecheck_cases, targets['Founta_hate'])]\n",
    "lime_df = pd.DataFrame(df_dict, index=ind)\n",
    "lime_df.columns = pd.MultiIndex.from_tuples(lime_df.columns, names=['value','Dataset'])\n",
    "lime_df.index = pd.MultiIndex.from_tuples(lime_df.index, names=['target', 'text'])\n",
    "pickle.dump(lime_df, open(\"Data/lime_scores.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_df = pickle.load(open(\"Data/lime_scores.pickle\", \"rb\"))\n",
    "shap_df = pickle.load(open(\"Data/shap_scores.pickle\", \"rb\"))\n",
    "master_df = pickle.load(open(\"Data/HateCheck_individual_necc_suff_scores.pickle\", \"rb\"))\n",
    "joint_df = pd.merge(master_df, shap_df, left_index=True, right_index=True)\n",
    "joint_df = pd.merge(joint_df, lime_df, left_index=True, right_index=True)\n",
    "joint_df = joint_df[joint_df[\"prediction\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97a4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_df['lime'].groupby(level='target').mean().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18977b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_df['shap'].groupby(level='target').std().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf31eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_df['lime'].groupby(level='target').std().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ecef03",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = {dd: joint_df.xs(dd, level='Dataset', axis=1)[['necessity', 'sufficiency', 'shap']].corr() for dd in datasets}\n",
    "df_concat = pd.concat([cc for cc in corrs.values()])\n",
    "by_row_index = df_concat.groupby(df_concat.index)\n",
    "df_means = by_row_index.mean()\n",
    "df_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = {dd: joint_df.xs(dd, level='Dataset', axis=1)[['necessity', 'sufficiency', 'lime']].corr() for dd in datasets}\n",
    "df_concat = pd.concat([cc for cc in corrs.values()])\n",
    "by_row_index = df_concat.groupby(df_concat.index)\n",
    "df_means = by_row_index.mean()\n",
    "df_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f227f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_df[('length', '')] = [len(tt.split()) for tt in joint_df.index.get_level_values(\"text\").to_list()]\n",
    "plot_df = pd.DataFrame()\n",
    "plot_df['length'] = joint_df[('length', '')]\n",
    "plot_df['shap'] = joint_df[('shap', 'Founta_hate')]\n",
    "plot_df['necessity'] = joint_df[('necessity', 'Founta_hate')]\n",
    "plot_df['sufficiency'] = joint_df[('sufficiency', 'Founta_hate')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f5e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lmplot(x=\"necessity\", y=\"length\", data=plot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b7cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lmplot(x=\"sufficiency\", y=\"length\", data=plot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed74d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lmplot(x=\"shap\", y=\"length\", data=plot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9261ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "founta_hate_df = joint_df.xs('Founta_hate', level='Dataset', axis=1)\n",
    "founta_hate_df[['necessity', 'sufficiency', 'shap']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c26d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "founta_hate_df.loc['Muslims'][['necessity', 'sufficiency', 'shap']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "founta_hate_df.loc['women'][['necessity', 'sufficiency', 'shap']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d67b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is Simpson's paradox! Lets plot to make sure this is what we are seeing. \n",
    "plot_df = founta_hate_df.copy()\n",
    "plot_df.index = founta_hate_df.index.get_level_values('text')\n",
    "plot_df['target'] = founta_hate_df.index.get_level_values('target')\n",
    "g = sns.lmplot(x=\"sufficiency\", y=\"shap\", hue=\"target\", data=plot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f1179",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lmplot(x=\"necessity\", y=\"sufficiency\", data=plot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fecc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "founta_hate_df[['necessity_mask', 'sufficiency_mask', 'shap']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2039045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "founta_hate_df[['necessity', 'necessity_mask']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109bf9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "founta_hate_df[['sufficiency', 'sufficiency_mask']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aee8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from perturbation_functions import get_preds_and_scores, calc_suff, calc_necc\n",
    "\n",
    "# re-calculate the necc and suff scores for Founta-hate as comparison\n",
    "perts = pickle.load(open(\"Data/HateCheck_necc_suff_perturbations_2.pickle\",\"rb\"))\n",
    "perts['orig_texts'] = [tt.strip(' \\n') for tt in perts['orig_texts']]\n",
    "dataset = \"Founta_hate\"\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# add special tokens for URLs, emojis and mentions (--> see pre-processing)\n",
    "special_tokens_dict = {'additional_special_tokens': ['[USER]','[EMOJI]','[URL]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "print(\"Classifying HateCheck perturbations with {}.\".format(dataset))\n",
    "#  model = BertForSequenceClassification.from_pretrained(models_dir +'BERT_{}_weighted/Final'.format(dataset))\n",
    "model = BertForSequenceClassification.from_pretrained(\"Models/Classifiers/{}\".format(dataset))\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.eval()\n",
    "\n",
    "total_len = len(perts['orig_texts']) + sum(len(nn) for nn in perts['necc_perturbed']) + sum(len(nn) for nn in perts['suff_perturbed'])\n",
    "\n",
    "with tqdm(total=total_len) as pbar:\n",
    "    orig_preds, orig_scores = get_preds_and_scores(perts['orig_texts'], tokenizer, model, pbar)\n",
    "\n",
    "    necc_preds = []\n",
    "    necc_scores = []\n",
    "\n",
    "    for tt in perts['necc_perturbed']:\n",
    "        pp, ss = get_preds_and_scores(tt, tokenizer, model, pbar)\n",
    "        necc_preds.append(pp)\n",
    "        necc_scores.append(ss)\n",
    "\n",
    "    suff_preds = []\n",
    "    suff_scores = []\n",
    "\n",
    "    for tt in perts['suff_perturbed']:\n",
    "        pp, ss = get_preds_and_scores(tt, tokenizer, model, pbar)\n",
    "        suff_preds.append(pp)\n",
    "        suff_scores.append(ss)\n",
    "\n",
    "\n",
    "Founta_hate_2_results = {\n",
    "            'orig_preds': orig_preds,\n",
    "            'orig_scores': orig_scores,\n",
    "            'necc_preds': necc_preds,\n",
    "            'necc_scores': necc_scores,\n",
    "            'suff_preds': suff_preds,\n",
    "            'suff_scores': suff_scores,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02475b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "necc_results_2 = []\n",
    "suff_results_2 = []\n",
    "baseline_preds = pickle.load(open(\"Data/Classifier_baselines.pickle\", \"rb\"))\n",
    "baseline_pred = baseline_preds['baseline_preds']['Founta_hate']\n",
    "\n",
    "## NECCESSITY CALCULATIONS\n",
    "for oo, pp, mm in zip(Founta_hate_2_results['orig_preds'], \n",
    "                      Founta_hate_2_results['necc_preds'], \n",
    "                      perts['necc_masks']):\n",
    "    if oo == 1:\n",
    "        pp = np.array(pp)\n",
    "        necc_results_2.append(calc_necc(oo, pp, mm))\n",
    "\n",
    "## SUFFICIENCY CALCULATIONS\n",
    "\n",
    "suffs = []\n",
    "for oo, pp, mm in zip(Founta_hate_2_results['orig_preds'],\n",
    "                      Founta_hate_2_results['suff_preds'], \n",
    "                      perts['suff_masks']):\n",
    "    if oo == 1:\n",
    "        pp = np.array(pp)\n",
    "        suff_results_2.append(calc_suff(baseline_pred, pp, mm))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_data = pd.read_csv(open(\"hatecheck-data/test_suite_cases.csv\"))\n",
    "hc_data.test_case = hc_data.test_case.apply(lambda tt: tt.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_templs = hc_data[['case_templ']]\n",
    "hc_index = hc_data[['test_case', 'target_ident']].rename(mapper={'test_case':'text', 'target_ident':'target'}, axis='columns')\n",
    "hc_templs.index = pd.MultiIndex.from_frame(hc_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ba9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "founta_hate_df = founta_hate_df.merge(hc_templs, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_placeholder(text):\n",
    "    text = text.strip().split()\n",
    "    for nn, tt in enumerate(text):\n",
    "        if tt[:1] == '[':\n",
    "            return nn\n",
    "        \n",
    "placeholder_locs = [find_placeholder(tt) for tt in founta_hate_df.case_templ.tolist()]\n",
    "founta_hate_df['necessity_2'] = [ll[nn] for ll, nn in zip(necc_results_2, placeholder_locs)]\n",
    "founta_hate_df['sufficiency_2'] = [ll[nn] for ll, nn in zip(suff_results_2, placeholder_locs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12094938",
   "metadata": {},
   "outputs": [],
   "source": [
    "founta_hate_df[['necessity', 'necessity_2']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba843bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "founta_hate_df[['sufficiency', 'sufficiency_2']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639b1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "founta_hate_df['suff_diff'] = (founta_hate_df['sufficiency'] - founta_hate_df['sufficiency_2']).abs()\n",
    "founta_hate_df['necc_diff'] = (founta_hate_df['necessity'] - founta_hate_df['necessity_2']).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd022484",
   "metadata": {},
   "outputs": [],
   "source": [
    "founta_hate_df['suff_diff'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f504715",
   "metadata": {},
   "outputs": [],
   "source": [
    "founta_hate_df['necc_diff'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
