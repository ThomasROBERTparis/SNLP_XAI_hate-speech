{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f6199fb",
   "metadata": {},
   "source": [
    "This notebook contains the the results for necessity and sufficiency. Necessity and sufficiency are both calculated by either choosing a subset of tokens and perturbing them using the ILM model. The models are all BERT architecture, but trained on different datasets, and for each dataset, a model is trained on both hate/non-hate and abusive/non-abusive labels. The explanations are generated for 120 examples from the HateCheck test suite. These are instances that are explicitly hateful, and are targeted towards women or Muslims. The function ```display_scores``` displays the necessity and sufficiency for each of the examples for all models included. Note that some models will display ```NaN``` for some values. These are the cases where the model mistakenly classified the original instance as non-abusive/non-hateful. In these cases, the current necessity and sufficiency calculations aren't meaningful, because we aim to provide explanations for positive predictions only. The third argument to this function determines which necessity/sufficiency scores to display. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e0c709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2021/thomas.robert.x21/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/users/eleves-b/2021/thomas.robert.x21/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bd92dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# preds = pickle.load(open(\"Data/HateCheck_necc_suff_preds.pickle\", \"rb\"))\n",
    "# results = pickle.load(open(\"Data/HateCheck_necc_suff_results_all.pickle\", \"rb\"))\n",
    "# perturbations = pickle.load(open(\"Data/intermediate outputs/HateCheck_necc_suff_perturbations.pickle\",\"rb\"))\n",
    "preds = pickle.load(open(\"Data/Reproduction/HateCheck_necc_suff_preds.pickle\", \"rb\"))\n",
    "results = pickle.load(open(\"Data/Reproduction/HateCheck_necc_suff_results_all.pickle\", \"rb\"))\n",
    "perturbations = pickle.load(open(\"Data/Reproduction/HateCheck_necc_suff_perturbations_0_20.pickle\",\"rb\"))\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cede3474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['orig_texts', 'necc_perturbed', 'suff_perturbed', 'necc_masks', 'suff_masks'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "396f30c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['orig_preds', 'orig_scores', 'necc_preds', 'necc_scores', 'suff_preds', 'suff_scores'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de5d9aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['necc_results', 'necc_results_nb', 'suff_results', 'suff_results_nb'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23010cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Davidson_hate']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = list(results['necc_results'].keys())\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "737feff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the corrupted examples with tokn k flipped together with the corresponding scores\n",
    "# if this is sufficiency, set reverse=True so that it will return instances where token k is not flipped\n",
    "def get_k_corr(k, masks, perturbed, p_results, reverse=False):\n",
    "    perturbed_k = []\n",
    "    for pp, mm, rr in zip(perturbed, masks[:,k], p_results):\n",
    "        if mm != reverse:\n",
    "            perturbed_k.append((pp, rr))\n",
    "    return(perturbed_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adaab936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given original test case, make a table for necessity or sufficiency for each model and for each token\n",
    "\n",
    "def display_scores(templ_n, orig_texts, orig_preds, scores_dict):\n",
    "    columns = orig_texts[templ_n].strip().split()\n",
    "    index = list(scores_dict.keys())\n",
    "    # if the model has not predicted the original as 0, do not display the feature attribution scores\n",
    "    data = np.array([scores_dict[dset][templ_n] if orig_preds[dset][templ_n] == 1 \n",
    "                     else [np.nan]*len(scores_dict[dset][templ_n]) \n",
    "                    for dset in scores_dict.keys()])\n",
    "    return pd.DataFrame(data=data, columns=columns, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c690f8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "necessity\n",
      "                     I  hate  women.\n",
      "Davidson_hate  0.42268   1.0     1.0\n",
      "                      I      hate  Catholics.\n",
      "Davidson_hate  0.217822  0.252747         1.0\n",
      "                      I  hate  men.\n",
      "Davidson_hate  0.417476   1.0   1.0\n",
      "                     I      hate  Muslims.\n",
      "Davidson_hate  0.27619  0.196078       1.0\n",
      "                I  really  can't  stand  women  like  you.\n",
      "Davidson_hate NaN     NaN    NaN    NaN    NaN   NaN   NaN\n",
      "                      I    really     can't     stand  Catholics      like  \\\n",
      "Davidson_hate  0.494737  0.563218  0.524752  0.495327   0.959596  0.587156   \n",
      "\n",
      "                   you.  \n",
      "Davidson_hate  0.646552  \n",
      "                I  really  can't  stand  men  like  you.\n",
      "Davidson_hate NaN     NaN    NaN    NaN  NaN   NaN   NaN\n",
      "                      I    really     can't     stand   Muslims      like  \\\n",
      "Davidson_hate  0.409091  0.421687  0.397959  0.478261  0.938144  0.515152   \n",
      "\n",
      "                   you.  \n",
      "Davidson_hate  0.569892  \n",
      "                I  despise  women.\n",
      "Davidson_hate NaN      NaN     NaN\n",
      "                     I  despise  Catholics.\n",
      "Davidson_hate  0.29703      0.2         1.0\n",
      "                I  despise  men.\n",
      "Davidson_hate NaN      NaN   NaN\n",
      "                      I   despise  Muslims.\n",
      "Davidson_hate  0.277228  0.267327       1.0\n",
      "                      I  detest  women.\n",
      "Davidson_hate  0.473118     1.0     1.0\n",
      "                  I    detest  Catholics.\n",
      "Davidson_hate  0.25  0.287234         1.0\n",
      "                I  detest  men.\n",
      "Davidson_hate NaN     NaN   NaN\n",
      "                  I    detest  Muslims.\n",
      "Davidson_hate  0.28  0.228571       1.0\n",
      "                I  absolutely  loathe  women.\n",
      "Davidson_hate NaN         NaN     NaN     NaN\n",
      "                      I  absolutely    loathe  Catholics.\n",
      "Davidson_hate  0.365385    0.495146  0.416667         1.0\n",
      "                I  absolutely  loathe  men.\n",
      "Davidson_hate NaN         NaN     NaN   NaN\n",
      "                      I  absolutely    loathe  Muslims.\n",
      "Davidson_hate  0.359223    0.359223  0.369369       1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"necessity\")\n",
    "for i in range(20):\n",
    "    print(display_scores(i, perturbations['orig_texts'],  preds['orig_preds'], results['necc_results']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f939f96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sufficiency\n",
      "                   I      hate    women.\n",
      "Davidson_hate -0.041  0.177391  0.147119\n",
      "                   I      hate  Catholics.\n",
      "Davidson_hate  0.239  0.203681       0.959\n",
      "                   I      hate      men.\n",
      "Davidson_hate -0.041  0.254918  0.254918\n",
      "                   I      hate  Muslims.\n",
      "Davidson_hate  0.179  0.191323     0.959\n",
      "                I  really  can't  stand  women  like  you.\n",
      "Davidson_hate NaN     NaN    NaN    NaN    NaN   NaN   NaN\n",
      "                      I    really     can't     stand  Catholics      like  \\\n",
      "Davidson_hate  0.295207  0.332737  0.321745  0.305939   0.762922  0.490915   \n",
      "\n",
      "                   you.  \n",
      "Davidson_hate  0.444714  \n",
      "                I  really  can't  stand  men  like  you.\n",
      "Davidson_hate NaN     NaN    NaN    NaN  NaN   NaN   NaN\n",
      "                      I    really     can't     stand   Muslims      like  \\\n",
      "Davidson_hate  0.285733  0.302434  0.361174  0.304455  0.775327  0.370215   \n",
      "\n",
      "                   you.  \n",
      "Davidson_hate  0.416447  \n",
      "                I  despise  women.\n",
      "Davidson_hate NaN      NaN     NaN\n",
      "                      I   despise  Catholics.\n",
      "Davidson_hate  0.228663  0.148189       0.959\n",
      "                I  despise  men.\n",
      "Davidson_hate NaN      NaN   NaN\n",
      "                      I   despise  Muslims.\n",
      "Davidson_hate  0.181222  0.229588     0.959\n",
      "                   I    detest    women.\n",
      "Davidson_hate -0.041  0.168091  0.198583\n",
      "                      I    detest  Catholics.\n",
      "Davidson_hate  0.282529  0.140818       0.959\n",
      "                I  detest  men.\n",
      "Davidson_hate NaN     NaN   NaN\n",
      "                      I    detest  Muslims.\n",
      "Davidson_hate  0.211632  0.197938     0.959\n",
      "                I  absolutely  loathe  women.\n",
      "Davidson_hate NaN         NaN     NaN     NaN\n",
      "                     I  absolutely    loathe  Catholics.\n",
      "Davidson_hate  0.25561    0.285087  0.264556       0.959\n",
      "                I  absolutely  loathe  men.\n",
      "Davidson_hate NaN         NaN     NaN   NaN\n",
      "                      I  absolutely  loathe  Muslims.\n",
      "Davidson_hate  0.240818     0.26203   0.259     0.959\n"
     ]
    }
   ],
   "source": [
    "print(\"sufficiency\")\n",
    "for i in range(20):\n",
    "    print(display_scores(i, perturbations['orig_texts'],  preds['orig_preds'], results['suff_results']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61e6f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This function is to examine the perturbations and the scores a model assigned them. \n",
    "# # Will not work for masked-not-perturbed versions. \n",
    "\n",
    "# ex_no = 0 # example in the examples list\n",
    "# id_token = 2  #the identity token is the _th token\n",
    "# model = 'Davidson_hate'\n",
    "# get_k_corr(id_token, \n",
    "#            perturbations['suff_masks'][ex_no], \n",
    "#            perturbations['suff_perturbed'][ex_no], \n",
    "#            preds['suff_scores'][model][ex_no], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0aaf1a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex_no = 0 # example in the examples list\n",
    "# id_token = 2  #the identity token is the _th token\n",
    "# model = 'CAD_abuse'\n",
    "# get_k_corr(id_token, \n",
    "#            perturbations['necc_masks'][ex_no], \n",
    "#            perturbations['necc_perturbed'][ex_no], \n",
    "#            preds['necc_scores'][model][ex_no])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c3a3c",
   "metadata": {},
   "source": [
    "In the following section, there is the average prediction of the models for the cases where identities are mentioned in neutral or positive contexts (```ident_neutral_nh```, ```ident_pos_nh```) and the test cases where there is \"abuse\" directed to non-protected groups (```target_group_nh```) individuals (```target_indiv_nh```) and objects (```target_obj_nh```). The performance of the models on non-abusive mention identities for the two targets we've chosen for our explanations ('women' and 'Muslims') is also given seperately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "016715eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['women', 'Catholics', 'trans people', 'men', 'gay people',\n",
       "       'black people', 'disabled people', 'Muslims', 'immigrants', nan],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "# hc_results = pickle.load(open('Data/intermediate outputs/HateCheck_templates_and_results.pickle', \"rb\"))\n",
    "hc_results = pickle.load(open('Data/Reproduction/HateCheck_templates_and_results.pickle', \"rb\"))\n",
    "###\n",
    "hc_results.test_case = hc_results.test_case.apply(lambda x: x.strip())\n",
    "hc_results.target_ident.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da4bcd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_results_women_nh = hc_results[((hc_results.functionality == 'ident_neutral_nh') \n",
    "                         | (hc_results.functionality == 'ident_pos_nh')) \n",
    "                        & (hc_results.target_ident == 'women')]\n",
    "\n",
    "hc_results_men_nh = hc_results[((hc_results.functionality == 'ident_neutral_nh') \n",
    "                         | (hc_results.functionality == 'ident_pos_nh')) \n",
    "                        & (hc_results.target_ident == 'men')]\n",
    "\n",
    "hc_results_muslims_nh = hc_results[((hc_results.functionality == 'ident_neutral_nh') \n",
    "                         | (hc_results.functionality == 'ident_pos_nh')) \n",
    "                        & (hc_results.target_ident == 'Muslim')]\n",
    "\n",
    "hc_results_catholics_nh = hc_results[((hc_results.functionality == 'ident_neutral_nh') \n",
    "                         | (hc_results.functionality == 'ident_pos_nh')) \n",
    "                        & (hc_results.target_ident == 'Catholic')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebf8aefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_205721/2736730449.py:1: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hc_results.functionality.loc[((hc_results.functionality == 'ident_neutral_nh')\n",
      "/tmp/ipykernel_205721/2736730449.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hc_results.functionality.loc[((hc_results.functionality == 'ident_neutral_nh')\n",
      "/tmp/ipykernel_205721/2736730449.py:5: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hc_results.functionality.loc[((hc_results.functionality == 'ident_neutral_nh')\n",
      "/tmp/ipykernel_205721/2736730449.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hc_results.functionality.loc[((hc_results.functionality == 'ident_neutral_nh')\n"
     ]
    }
   ],
   "source": [
    "hc_results.functionality.loc[((hc_results.functionality == 'ident_neutral_nh') \n",
    "                         | (hc_results.functionality == 'ident_pos_nh')) \n",
    "                        & (hc_results.target_ident == 'women')] = 'women_nh'\n",
    "\n",
    "hc_results.functionality.loc[((hc_results.functionality == 'ident_neutral_nh') \n",
    "                         | (hc_results.functionality == 'ident_pos_nh')) \n",
    "                        & (hc_results.target_ident == 'men')] = 'men_nh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd5e56d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_205721/922294377.py:1: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hc_results.functionality.loc[((hc_results.functionality == 'ident_neutral_nh')\n",
      "/tmp/ipykernel_205721/922294377.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hc_results.functionality.loc[((hc_results.functionality == 'ident_neutral_nh')\n",
      "/tmp/ipykernel_205721/922294377.py:5: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  hc_results.functionality.loc[((hc_results.functionality == 'ident_neutral_nh')\n",
      "/tmp/ipykernel_205721/922294377.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  hc_results.functionality.loc[((hc_results.functionality == 'ident_neutral_nh')\n"
     ]
    }
   ],
   "source": [
    "hc_results.functionality.loc[((hc_results.functionality == 'ident_neutral_nh') \n",
    "                         | (hc_results.functionality == 'ident_pos_nh')) \n",
    "                        & (hc_results.target_ident == 'Muslims')] = 'muslims_nh'\n",
    "\n",
    "hc_results.functionality.loc[((hc_results.functionality == 'ident_neutral_nh') \n",
    "                         | (hc_results.functionality == 'ident_pos_nh')) \n",
    "                        & (hc_results.target_ident == 'Catholics')] = 'catholics_nh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27a08913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>functionality</th>\n",
       "      <th>catholics_nh</th>\n",
       "      <th>men_nh</th>\n",
       "      <th>muslims_nh</th>\n",
       "      <th>target_group_nh</th>\n",
       "      <th>target_indiv_nh</th>\n",
       "      <th>target_obj_nh</th>\n",
       "      <th>women_nh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Davidson_hate_pred</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.467742</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "functionality       catholics_nh  men_nh  muslims_nh  target_group_nh  \\\n",
       "Davidson_hate_pred      0.666667     0.0         0.8         0.467742   \n",
       "\n",
       "functionality       target_indiv_nh  target_obj_nh  women_nh  \n",
       "Davidson_hate_pred              0.6       0.092308       0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the results we are interested are: \n",
    "target_funcs = ['women_nh', 'men_nh', 'muslims_nh', 'catholics_nh', 'target_obj_nh', 'target_indiv_nh', 'target_group_nh']\n",
    "\n",
    "target_funcs_results = hc_results[hc_results.functionality.isin(target_funcs)]\n",
    "# get average score per functionality\n",
    "target_funcs_results.groupby('functionality')[['{}_pred'.format(dd) for dd in datasets]].mean().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e35d9711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['necc_results', 'necc_results_nb', 'suff_results', 'suff_results_nb'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_results = pickle.load(open('Data/intermediate outputs/HateCheck_necc_suff_results_masked.pickle', 'rb'))\n",
    "mask_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8566e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "necc_vals = {}\n",
    "suff_vals = {}\n",
    "necc_vals_mask = {}\n",
    "suff_vals_mask = {}\n",
    "orig_texts = []\n",
    "targets = []\n",
    "\n",
    "for tt in perturbations['orig_texts']:\n",
    "    orig_text = tt.strip()\n",
    "    row = hc_results[hc_results.test_case == orig_text]\n",
    "    targets.append(row.target_ident.tolist()[0])\n",
    "\n",
    "for dataset in datasets:\n",
    "    necc_vals[dataset] = []\n",
    "    suff_vals[dataset] = []\n",
    "    necc_vals_mask[dataset] = []\n",
    "    suff_vals_mask[dataset] = []\n",
    "    for nn, (orig_text, orig_pred) in enumerate(zip(perturbations['orig_texts'], preds['orig_preds'][dataset])):\n",
    "        if orig_pred != 1:\n",
    "            necc_vals[dataset].append(np.nan)\n",
    "            suff_vals[dataset].append(np.nan)\n",
    "            necc_vals_mask[dataset].append(np.nan)\n",
    "            suff_vals_mask[dataset].append(np.nan)\n",
    "            continue\n",
    "        # get the row in hc_results corresponding to this case\n",
    "        orig_text = orig_text.strip()\n",
    "        row = hc_results[hc_results.test_case == orig_text]\n",
    "        toknd = row.case_templ.tolist()[0].split()\n",
    "        ## find the index of the template placeholder\n",
    "        for ii, tt in enumerate(toknd):\n",
    "            if tt[:1] == \"[\":\n",
    "                break\n",
    "        necc_vals[dataset].append(results['necc_results'][dataset][nn][ii])\n",
    "        suff_vals[dataset].append(results['suff_results'][dataset][nn][ii])\n",
    "        # necc_vals_mask[dataset].append(mask_results['necc_results_nb'][dataset][nn][ii])\n",
    "        # suff_vals_mask[dataset].append(mask_results['suff_results_nb'][dataset][nn][ii])\n",
    "\n",
    "df_dict = {('necessity', dd): ll for dd, ll in necc_vals.items()}\n",
    "df_dict.update({('sufficiency', dd): ll for dd, ll in suff_vals.items()})\n",
    "# df_dict.update({('necessity_mask', dd): ll for dd, ll in necc_vals_mask.items()})\n",
    "# df_dict.update({('sufficiency_mask', dd): ll for dd, ll in suff_vals_mask.items()})\n",
    "df_dict.update({('prediction', dd): ll for dd, ll in preds['orig_preds'].items()})\n",
    "df_dict.update({('score', dd): ll for dd, ll in preds['orig_scores'].items()})\n",
    "#df_dict.update({'target', ''}: targets)\n",
    "\n",
    "#ind = [xx.strip() for xx in perturbations['orig_texts']]\n",
    "ind = [(tt, xx.strip()) for xx, tt in zip(perturbations['orig_texts'], targets)]\n",
    "\n",
    "# pd.DataFrame(df_dict, index=ind)\n",
    "#     avg_necc[dataset] = {target: np.mean(necc_vals[target]) for target in targets}\n",
    "#     avg_suff[dataset] = {target: np.mean(suff_vals[target]) for target in targets}\n",
    "\n",
    "master_df = pd.DataFrame(df_dict, index=ind)\n",
    "master_df.columns = pd.MultiIndex.from_tuples(master_df.columns, names=['value','Dataset'])\n",
    "master_df.index = pd.MultiIndex.from_tuples(master_df.index, names=['target', 'text'])\n",
    "pickle.dump(master_df, open(\"Data/Reproduction/HateCheck_individual_necc_suff_scores.pickle\", \"wb\"))\n",
    "\n",
    "# master_df.xs('CAD_abuse', level='Dataset', axis=1)\n",
    "# master_df['necessity']\n",
    "# master_df.loc['women']\n",
    "# master_df.xs('I hate women.', level='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c950d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pickle.load(open(\"Data/Reproduction/HateCheck_individual_necc_suff_scores.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c718aba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>target</th>\n",
       "      <th>Catholics</th>\n",
       "      <th>Muslims</th>\n",
       "      <th>men</th>\n",
       "      <th>women</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Davidson_hate</th>\n",
       "      <td>0.991919</td>\n",
       "      <td>0.987629</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "target         Catholics   Muslims  men  women\n",
       "Dataset                                       \n",
       "Davidson_hate   0.991919  0.987629  1.0    1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df['necessity'].groupby(level='target').mean().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75f8cb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>target</th>\n",
       "      <th>Catholics</th>\n",
       "      <th>Muslims</th>\n",
       "      <th>men</th>\n",
       "      <th>women</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Davidson_hate</th>\n",
       "      <td>0.018069</td>\n",
       "      <td>0.027663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "target         Catholics   Muslims  men  women\n",
       "Dataset                                       \n",
       "Davidson_hate   0.018069  0.027663  NaN    0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df['necessity'].groupby(level='target').std().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bd4f0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>target</th>\n",
       "      <th>Catholics</th>\n",
       "      <th>Muslims</th>\n",
       "      <th>men</th>\n",
       "      <th>women</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Davidson_hate</th>\n",
       "      <td>0.919784</td>\n",
       "      <td>0.922265</td>\n",
       "      <td>0.254918</td>\n",
       "      <td>0.172851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "target         Catholics   Muslims       men     women\n",
       "Dataset                                               \n",
       "Davidson_hate   0.919784  0.922265  0.254918  0.172851"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df['sufficiency'].groupby(level='target').mean().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dabd51ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>target</th>\n",
       "      <th>Catholics</th>\n",
       "      <th>Muslims</th>\n",
       "      <th>men</th>\n",
       "      <th>women</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Davidson_hate</th>\n",
       "      <td>0.087689</td>\n",
       "      <td>0.082141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.036391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "target         Catholics   Muslims  men     women\n",
       "Dataset                                          \n",
       "Davidson_hate   0.087689  0.082141  NaN  0.036391"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df['sufficiency'].groupby(level='target').std().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05ef656b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'necessity_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'necessity_mask'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmaster_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnecessity_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby(level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mtranspose()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py:4089\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4087\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_single_key:\n\u001b[1;32m   4088\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 4089\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_multilevel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4090\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4091\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py:4147\u001b[0m, in \u001b[0;36mDataFrame._getitem_multilevel\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getitem_multilevel\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   4146\u001b[0m     \u001b[38;5;66;03m# self.columns is a MultiIndex\u001b[39;00m\n\u001b[0;32m-> 4147\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, (\u001b[38;5;28mslice\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray)):\n\u001b[1;32m   4149\u001b[0m         new_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[loc]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/indexes/multi.py:3040\u001b[0m, in \u001b[0;36mMultiIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n\u001b[1;32m   3039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m-> 3040\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_level_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _maybe_to_slice(loc)\n\u001b[1;32m   3043\u001b[0m keylen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(key)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/indexes/multi.py:3391\u001b[0m, in \u001b[0;36mMultiIndex._get_level_indexer\u001b[0;34m(self, key, level, indexer)\u001b[0m\n\u001b[1;32m   3388\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(i, j, step)\n\u001b[1;32m   3390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3391\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_loc_single_level_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lexsort_depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3394\u001b[0m         \u001b[38;5;66;03m# Desired level is not sorted\u001b[39;00m\n\u001b[1;32m   3395\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   3396\u001b[0m             \u001b[38;5;66;03m# test_get_loc_partial_timestamp_multiindex\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/indexes/multi.py:2980\u001b[0m, in \u001b[0;36mMultiIndex._get_loc_single_level_index\u001b[0;34m(self, level_index, key)\u001b[0m\n\u001b[1;32m   2978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2979\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlevel_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'necessity_mask'"
     ]
    }
   ],
   "source": [
    "master_df['necessity_mask'].groupby(level='target').mean().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032913e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df['sufficiency_mask'].groupby(level='target').mean().transpose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
